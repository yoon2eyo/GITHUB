# 4.3.2 Node Specification

## Overview
본 문서는 Smart Fitness Management System의 Deployment Diagram에 표시된 모든 노드의 상세 사양과 역할을 기술합니다. 각 노드는 시스템의 기능 및 품질 속성(QA) 달성에 기여하며, 다중성(Multiplicity)이 있는 경우 각 인스턴스의 역할을 명확히 합니다.

각 노드에는 배포되는 컴포넌트들이 명시되어 있으며, 이는 Component Diagram에서 정의된 구조를 반영합니다.

---

## Client Zone Nodes

| Name | Description |
|------|-------------|
| **Customer Mobile Device** | **역할**: 고객이 사용하는 모바일 디바이스로, 안면 출입, 지점 검색, 리뷰 등록 기능 제공<br>**Multiplicity**: N개 (등록 사용자 수에 비례, 예상 10,000+)<br>**H/W 사양**: 스마트폰 (iOS 14+ / Android 10+), 최소 2GB RAM, 카메라 탑재<br>**S/W 사양**: Customer App (React Native / Flutter), TLS 1.3 지원<br>**QA 기여**:<br>- Security (QAS-04): TLS 1.3 암호화 통신, Certificate Pinning으로 중간자 공격 방지<br>- Usability: 모바일 최적화 UI/UX, 오프라인 캐싱<br>**역할 상세**: 안면 사진 촬영 및 서버 전송, 지점 검색 결과 표시, 리뷰 작성 인터페이스 제공 |
| **Helper Mobile Device** | **역할**: 헬퍼가 사용하는 모바일 디바이스로, 작업 등록, 보상 조회 기능 제공<br>**Multiplicity**: N개 (헬퍼 수에 비례, 예상 1,000+)<br>**H/W 사양**: 스마트폰 (iOS 14+ / Android 10+), 최소 2GB RAM, 카메라 탑재<br>**S/W 사양**: Helper App (React Native / Flutter), TLS 1.3 지원<br>**QA 기여**:<br>- Security (QAS-04): TLS 암호화 통신, 작업 사진 암호화 업로드<br>- Performance: 로컬 사진 압축으로 업로드 시간 단축<br>**역할 상세**: 세탁물 작업 사진 촬영 및 S3 업로드, 작업 상태 및 보상 잔고 조회 |
| **Manager Mobile Device** | **역할**: 지점주가 사용하는 모바일 디바이스로, 작업 검수, 설비 모니터링, 알림 수신 기능 제공<br>**Multiplicity**: N개 (지점주 수에 비례, 예상 1,000+)<br>**H/W 사양**: 스마트폰 (iOS 14+ / Android 10+), 최소 2GB RAM<br>**S/W 사양**: Manager App (React Native / Flutter), FCM Push SDK, TLS 1.3 지원<br>**QA 기여**:<br>- Availability (QAS-01): FCM Push로 설비 고장 알림 실시간 수신 (< 15초)<br>- Security (QAS-04): TLS 암호화, Role-Based Access Control (지점주 권한)<br>**역할 상세**: AI 판독 결과 검수, 설비 상태 모니터링, 고장 알림 수신 및 대응 |
| **Operations Workstation** | **역할**: 운영팀이 사용하는 데스크톱/노트북으로, 시스템 모니터링 및 MLOps 관리 대시보드 제공<br>**Multiplicity**: 소수 (운영팀 규모, 예상 5-10대)<br>**H/W 사양**: Desktop/Laptop (Windows/macOS/Linux), 최소 8GB RAM, 1920×1080 이상 해상도<br>**S/W 사양**: Web Browser (Chrome 90+, Firefox 88+), Admin Dashboard (React SPA)<br>**QA 기여**:<br>- Security (QAS-04): MFA (Multi-Factor Authentication), IP Whitelist, Admin RBAC<br>- Modifiability (QAS-06): MLOps 대시보드로 모델 배포/롤백 UI 제공<br>**역할 상세**: 시스템 메트릭 모니터링 (Grafana), 로그 분석 (Kibana), 모델 재학습 트리거, 배포 승인 |

---

## Branch Zone Nodes

| Name | Description |
|------|-------------|
| **Branch Equipment Node** | **역할**: 각 지점에 설치된 IoT 설비로, 안면 촬영, 게이트 제어, 상태 보고 기능 수행<br>**Multiplicity**: 1,000+ (각 지점당 1개, 1,000개 지점 가정)<br>**H/W 사양**:<br>- CPU: ARM Cortex-A53 (4 cores, 1.4GHz) 또는 Intel Atom x5<br>- Memory: 2GB DDR3 RAM<br>- Storage: 16GB eMMC (로컬 버퍼링용)<br>- Network: Ethernet (100Mbps) / WiFi (802.11ac)<br>- Camera: 1080p HD Camera (안면 인식용)<br>- GPIO: Gate Controller 연동 (Relay 제어)<br>- Sensors: Temperature, Status LED<br>**S/W 사양**:<br>- OS: Embedded Linux (Raspberry Pi OS / Yocto Linux)<br>- Runtime: Python 3.10+ / C++ (실시간 제어)<br>- Libraries: OpenCV (이미지 처리), Requests (HTTP 통신)<br>**QA 기여**:<br>- Performance (QAS-02): 로컬 이미지 전처리 (리사이징, 압축)로 네트워크 전송 시간 단축<br>- Availability (QAS-01): Heartbeat 10분 주기, Ping/Echo 응답으로 장애 감지 지원<br>- Availability: 로컬 버퍼링으로 일시적 네트워크 장애 시 데이터 손실 방지<br>**역할 상세**:<br>- **Camera Controller**: 사용자 접근 시 안면 사진 촬영 → HTTPS로 Access Service에 전송<br>- **Gate Controller**: Access Service로부터 게이트 개방 명령 수신 → Relay 신호로 물리적 게이트 제어<br>- **IoT Sensor Manager**: 설비 상태 (온도, 전원, 오류) 수집 → TCP Heartbeat로 Monitoring Service에 보고 (10분 주기)<br>**다중성 역할**: 각 인스턴스는 독립적으로 동작하며, 한 지점의 설비 장애가 다른 지점에 영향 없음 (Isolated Failure) |

---

## External Service Nodes

| Name | Description |
|------|-------------|
| **Credit Card Verification Service** | **역할**: 외부 PG사가 제공하는 신용카드 본인 인증 서비스<br>**Multiplicity**: 1 (외부 서비스 Provider)<br>**Provider**: NicePay, KCP, Inicis 등 국내 PG사<br>**Communication**: Auth Service → HTTPS REST API<br>**QA 기여**:<br>- Security (QAS-04): PCI-DSS Level 1 준수, 카드 정보 암호화 (AES-256), 본인 인증 (SMS/ARS)<br>- Availability: 99.9% SLA, 외부 장애 시 Circuit Breaker로 빠른 실패 및 폴백<br>**역할 상세**: 고객 회원가입 시 신용카드 정보 검증 → 본인 확인 후 승인/거부 응답 반환 |
| **LLM Service Provider** | **역할**: 상용 대형 언어 모델(LLM) API 서비스로, 자연어 분석 제공<br>**Multiplicity**: 1 (외부 서비스 Provider)<br>**Provider**: OpenAI (GPT-4), Anthropic (Claude 3)<br>**Communication**: Search Service → HTTPS REST API (Cold Path만 사용)<br>**QA 기여**:<br>- Performance (QAS-03): **Hot Path에서 제거**하여 실시간 검색 성능 보장 (< 3초)<br>- Cost Optimization: Cold Path (콘텐츠 등록 시)만 사용으로 API 호출 비용 90% 절감<br>- Modifiability (DD-09): Hot/Cold Path 분리로 LLM Provider 교체 용이<br>**역할 상세**: 지점 정보 또는 리뷰 등록 시 텍스트 분석 → 키워드 추출 및 성향 분석 → ElasticSearch 인덱싱 (비동기 배치 처리) |
| **FCM Server** | **역할**: Google Firebase Cloud Messaging 서버로, 모바일 푸시 알림 전송<br>**Multiplicity**: 1 (외부 서비스 Provider)<br>**Provider**: Google Firebase<br>**Communication**: Notification Service → HTTPS (FCM API), FCM Server → Manager App (FCM Push)<br>**QA 기여**:<br>- Availability (QAS-01): 푸시 알림으로 설비 고장 실시간 전달 (< 15초)<br>- Availability: 99.95% SLA, 알림 재시도 메커니즘 (최대 3회)<br>**역할 상세**: Notification Service가 알림 요청 → FCM Server가 지점주 디바이스에 Push 전송 → 디바이스 알림 표시 |

---

## Kubernetes Cluster Nodes

| Name | Description |
|------|-------------|
| **Load Balancer Node** | **역할**: 외부 클라이언트 요청을 API Gateway로 분산하는 L7 로드 밸런서<br>**Multiplicity**: 2+ (Active-Active HA 구성)<br>**H/W 사양** (per instance):<br>- CPU: 2 vCPU<br>- Memory: 2GB<br>- Network: 10Gbps (Elastic Load Balancer)<br>**S/W 사양**: Nginx Ingress Controller 1.9+ on Kubernetes Service (Type: LoadBalancer)<br>**배포 컴포넌트**:<br>- Nginx Ingress Controller<br>**QA 기여**:<br>- Availability (QAS-05): Active-Active 구성으로 단일 장애점(SPOF) 제거, 한 인스턴스 장애 시 자동 페일오버<br>- Performance: L7 로드 밸런싱으로 가중치 기반 트래픽 분산 (Weighted Round Robin)<br>- Security (QAS-04): TLS 1.3 Termination, DDoS 방어 (Rate Limiting: 초당 1,000 req/IP)<br>**역할 상세**: HTTPS 요청 수신 → TLS 복호화 → 헬스체크로 정상 API Gateway 인스턴스 선택 → HTTP로 전달<br>**다중성 역할**: 각 인스턴스가 동일한 부하 처리, 한 인스턴스 장애 시 다른 인스턴스가 즉시 트래픽 인수 |
| **API Gateway Node** | **역할**: 모든 클라이언트 요청의 단일 진입점, 인증/인가, 라우팅, Circuit Breaker 수행<br>**Multiplicity**: 3 replicas<br>**H/W 사양** (per pod):<br>- CPU: 1 vCPU<br>- Memory: 1GB<br>- Network: 10Gbps (Pod network)<br>**S/W 사양**: Spring Boot 3.2.x, Java 17, Spring Cloud Gateway, Eureka Client, Resilience4j<br>**배포 컴포넌트** (Interface Layer):<br>- ApiGatewayController, ApiGatewayManagementController<br>**배포 컴포넌트** (Business Layer):<br>- RequestRouter, SecurityManager, ServiceDiscoveryManager, LoadBalancer<br>- AuthenticationManager, AuthorizationManager, RequestSignatureVerifier<br>**배포 컴포넌트** (System Interface Layer):<br>- EurekaServiceRegistry, AuthenticationClientAdapter, AuthorizationClientAdapter<br>- RabbitMQAdapter, ResilientCircuitBreaker, ResilientRateLimiter<br>**QA 기여**:<br>- Availability (QAS-05): Circuit Breaker로 장애 전파 차단, Retry 메커니즘 (최대 3회), 타임아웃 설정 (5초)<br>- Security (QAS-04): JWT 토큰 검증 (RS256), Rate Limiting (사용자당 분당 100 req), IP Whitelist (관리자)<br>- Performance: Service Discovery (Eureka)로 동적 라우팅, Connection Pooling<br>**역할 상세**: 요청 수신 → JWT 검증 → Rate Limit 체크 → Eureka에서 대상 서비스 인스턴스 조회 → HTTP 라우팅 → 응답 반환<br>**다중성 역할**: 3개 인스턴스가 부하 분산 처리, 한 인스턴스 장애 시 Kubernetes가 자동 재시작 (< 30초) |
| **Real-Time Access Node (Co-located)** | **역할**: Access Service와 FaceModel Service가 동일 Pod에 공존하여 초저지연 출입 인증 수행<br>**Multiplicity**: 2 replicas<br>**H/W 사양** (per pod):<br>- CPU: 8 vCPU<br>- Memory: 8GB (Face Vector Cache 포함)<br>- Storage: 1GB (로컬 임시 저장)<br>- Network: 10Gbps<br>**S/W 사양**: Spring Boot 3.2.x, Java 17, gRPC 1.58+, Redis Client, PostgreSQL JDBC Driver<br>**Co-location 전략**: Kubernetes Pod Affinity로 Access + FaceModel을 동일 Pod 내 배치<br>**배포 컴포넌트** (Access Service - Interface Layer):<br>- AccessControlController, QRAccessController<br>**배포 컴포넌트** (Access Service - Business Layer):<br>- AccessAuthorizationManager, GateController, FaceVectorCache, AccessEventProcessor<br>**배포 컴포넌트** (Access Service - System Interface Layer):<br>- VectorRepository, FaceModelServiceIPCClient, EquipmentGatewayAdapter, RabbitMQAdapter<br>**배포 컴포넌트** (FaceModel Service - Interface Layer):<br>- FaceModelIPCHandler<br>**배포 컴포넌트** (FaceModel Service - Business Layer):<br>- VectorComparisonEngine, ModelLifecycleManager, FeatureExtractor<br>**배포 컴포넌트** (FaceModel Service - System Interface Layer):<br>- ModelVersionJpaRepository, MLInferenceEngineAdapter, RabbitMQAdapter<br>**QA 기여**:<br>- **Performance (QAS-02)**: **핵심 성능 최적화 노드**<br>  - **IPC/gRPC**: Access ↔ FaceModel 통신이 동일 Pod 내 Local Socket으로 처리 (< 5-10ms, HTTP 대비 10-20ms 단축)<br>  - **Face Vector Cache (Redis)**: 상위 10,000개 안면 벡터를 메모리 캐싱 (90%+ 히트율), DB 조회 50ms → 5ms (90% 감소)<br>  - **Pipeline Optimization**: FaceModel의 CompletableFuture 병렬 특징점 추출로 응답 시간 405ms → 205ms (49% 단축)<br>  - **결과**: 평균 응답 시간 ~230ms << 3초 목표 달성<br>- Availability: 2 replicas로 HA 보장, Health check (Liveness/Readiness Probe)<br>**역할 상세**:<br>- **Access Service**: 안면 사진 수신 → Face Vector Cache 조회 → IPC/gRPC로 FaceModel Service 호출 → 유사도 결과 수신 → 게이트 제어 → 출입 로그 저장<br>- **FaceModel Service**: IPC/gRPC 요청 수신 → ML Inference Engine으로 특징점 추출 (병렬) → 코사인 유사도 계산 → 결과 반환<br>**다중성 역할**: 2개 인스턴스가 부하 분산, 한 인스턴스 장애 시 다른 인스턴스가 트래픽 처리 (99.9% 가용성) |
| **Auth Service Node** | **역할**: 사용자 인증/인가, 회원가입, JWT 발급 수행<br>**Multiplicity**: 2 replicas<br>**H/W 사양** (per pod): CPU: 1-2 vCPU, Memory: 1-2GB<br>**S/W 사양**: Spring Boot 3.2.x, Java 17, Spring Security, JWT Library (jjwt)<br>**배포 컴포넌트** (Interface Layer):<br>- AuthServiceController, UserManagementController<br>**배포 컴포넌트** (Business Layer):<br>- AuthenticationManager, AuthorizationManager, UserRegistrationManager, AuthEventConsumer<br>**배포 컴포넌트** (System Interface Layer):<br>- AuthJpaRepository, RabbitMQAdapter, JwtTokenManager, CreditCardVerificationClient<br>**QA 기여**:<br>- Security (QAS-04): JWT 기반 인증 (RS256 서명), 비밀번호 암호화 (BCrypt), 안면 벡터 저장 시 암호화<br>- Availability: 2 replicas로 HA, Connection Pool (HikariCP: 10-20 connections)<br>**역할 상세**: 회원가입 요청 → 신용카드 검증 (외부 API) → 안면 사진 S3 업로드 → 특징점 추출 → 벡터 DB 저장 → JWT 발급<br>**다중성 역할**: 각 인스턴스가 독립적으로 인증 처리, DB 트랜잭션 경합 최소화 (Read Replica 활용) |
| **Helper Service Node** | **역할**: 헬퍼 작업 등록, AI 판독 요청, 보상 관리 수행<br>**Multiplicity**: 2 replicas<br>**H/W 사양** (per pod): CPU: 1-2 vCPU, Memory: 1-2GB<br>**S/W 사양**: Spring Boot 3.2.x, Java 17, AWS S3 SDK, RabbitMQ Client<br>**배포 컴포넌트** (Interface Layer):<br>- TaskController, RewardController<br>**배포 컴포넌트** (Business Layer):<br>- TaskSubmissionManager, DailyLimitValidator<br>- AITaskAnalysisConsumer, TaskAnalysisEngine<br>- RewardConfirmationManager, RewardUpdateConsumer, RewardCalculator<br>**배포 컴포넌트** (System Interface Layer):<br>- HelperJpaRepository, S3PhotoStorage, MLInferenceEngineAdapter, RabbitMQAdapter<br>**QA 기여**:<br>- Modifiability (QAS-06): 이벤트 기반 (TaskConfirmedEvent 발행) → MLOps Service 재학습 트리거<br>- Availability: 2 replicas, S3 업로드 재시도 메커니즘 (최대 3회)<br>**역할 상세**: 작업 사진 업로드 → S3 저장 → TaskSubmittedEvent 발행 → RabbitMQ → AI 분석 대기<br>**다중성 역할**: 각 인스턴스가 독립적으로 작업 처리, S3 업로드 병렬 처리 |
| **Search Service Node** | **역할**: 자연어 지점 검색 (Hot Path), 콘텐츠 인덱싱 (Cold Path) 수행<br>**Multiplicity**: 3 replicas (높은 검색 트래픽 대비)<br>**H/W 사양** (per pod): CPU: 1-2 vCPU, Memory: 1-2GB<br>**S/W 사양**: Spring Boot 3.2.x, Java 17, ElasticSearch Java Client, RabbitMQ Client<br>**배포 컴포넌트** (Interface Layer):<br>- BranchSearchController, ReviewController<br>**배포 컴포넌트** (Business Layer - Hot Path):<br>- SearchQueryManager, SimpleKeywordTokenizer, SearchEngineAdapter<br>**배포 컴포넌트** (Business Layer - Cold Path):<br>- ContentRegistrationManager, PreferenceAnalyzer, PreferenceMatchConsumer<br>**배포 컴포넌트** (System Interface Layer):<br>- ElasticSearchRepository, LLMServiceClient, RabbitMQAdapter<br>**QA 기여**:<br>- **Performance (QAS-03)**: **Hot/Cold Path 분리 (DD-09)**<br>  - **Hot Path**: Simple Tokenizer + ElasticSearch만 사용 (LLM 제거) → 평균 응답 시간 ~70ms << 3초<br>  - **Cold Path**: 비동기 LLM 분석 (콘텐츠 등록 시) → 검색 품질 향상 (실시간 성능 영향 없음)<br>- Scalability: 3 replicas로 높은 검색 트래픽 처리 (초당 100+ req)<br>**역할 상세**:<br>- **Hot Path**: 검색 요청 → Simple Tokenizer (키워드 추출, 5ms) → ElasticSearch 쿼리 (~50ms) → 결과 반환<br>- **Cold Path**: 콘텐츠 등록 이벤트 수신 → LLM API 호출 (비동기) → 키워드 추출 → ElasticSearch 인덱싱<br>**다중성 역할**: 3개 인스턴스가 검색 부하 분산, ElasticSearch 클러스터에 병렬 쿼리 |
| **BranchOwner Service Node** | **역할**: 지점 정보 관리, 작업 검수/컨펌 수행<br>**Multiplicity**: 2 replicas<br>**H/W 사양** (per pod): CPU: 1-2 vCPU, Memory: 1-2GB<br>**S/W 사양**: Spring Boot 3.2.x, Java 17, RabbitMQ Client<br>**배포 컴포넌트** (Interface Layer):<br>- BranchOwnerController, BranchQueryController<br>**배포 컴포넌트** (Business Layer):<br>- BranchOwnerManager, BranchInfoValidator, BranchEventProcessor<br>**배포 컴포넌트** (System Interface Layer):<br>- BranchJpaRepository, AuthJpaRepository, RabbitMQAdapter<br>**QA 기여**:<br>- Modifiability (QAS-06): TaskConfirmedEvent 발행 → Helper Service (보상 갱신) 및 MLOps Service (재학습) 트리거<br>- Availability: 2 replicas, 이벤트 발행 실패 시 재시도<br>**역할 상세**: 작업 검수 요청 → AI 판독 결과 표시 → 지점주 컨펌 → TaskConfirmedEvent 발행 → RabbitMQ<br>**다중성 역할**: 각 인스턴스가 독립적으로 지점주 요청 처리 |
| **Monitoring Service Node** | **역할**: 설비 상태 모니터링, 장애 감지 (Heartbeat + Ping/Echo) 수행<br>**Multiplicity**: 2 replicas<br>**H/W 사양** (per pod): CPU: 1-2 vCPU, Memory: 1-2GB<br>**S/W 사양**: Spring Boot 3.2.x, Java 17, Quartz Scheduler, RabbitMQ Client<br>**배포 컴포넌트** (Interface Layer):<br>- EquipmentStatusReceiver, EquipmentCommandController<br>**배포 컴포넌트** (Business Layer):<br>- HeartbeatReceiver, FaultDetector<br>- EquipmentHealthChecker, PingEchoExecutor<br>- AuditLogger<br>**배포 컴포넌트** (System Interface Layer):<br>- EquipmentStatusJpaRepository, EquipmentGatewayClient, QuartzScheduler, RabbitMQAdapter<br>**QA 기여**:<br>- **Availability (QAS-01)**: **Two-Level Fault Detection (DD-04)**<br>  - **Heartbeat**: 설비가 10분마다 상태 보고 → 즉시 장애 감지 (< 5초)<br>  - **Ping/Echo**: 10초마다 능동 점검 → 30초 무응답 시 Ping 전송 → 타임아웃 시 장애 확정<br>  - **결과**: P95 < 15초, P99 < 30초 알림 목표 달성<br>- Availability: Quartz Scheduler 단일 인스턴스 실행 (분산 스케줄링 방지), 다른 인스턴스는 Standby<br>**역할 상세**: Quartz Scheduler가 10초마다 트리거 → 모든 설비 최근 Heartbeat 확인 → 30초 초과 시 Ping 전송 → 무응답 시 EquipmentFaultEvent 발행<br>**다중성 역할**: Primary 인스턴스만 스케줄러 실행, 장애 시 Standby가 즉시 인수 (Quartz Cluster 모드) |
| **Notification Service Node** | **역할**: 이벤트 기반 푸시 알림 발송 (FCM)<br>**Multiplicity**: 3 replicas (높은 알림 부하 대비)<br>**H/W 사양** (per pod): CPU: 1-2 vCPU, Memory: 1-2GB<br>**S/W 사양**: Spring Boot 3.2.x, Java 17, FCM Admin SDK, RabbitMQ Client<br>**배포 컴포넌트** (Interface Layer):<br>- NotificationController<br>**배포 컴포넌트** (Business Layer):<br>- NotificationDispatcherManager, NotificationDispatcherConsumer<br>**배포 컴포넌트** (System Interface Layer):<br>- FcmPushGateway, RabbitMQAdapter<br>**QA 기여**:<br>- Availability (QAS-01): 이벤트 구독 (EquipmentFaultEvent) → FCM Push → 지점주에게 실시간 알림 (< 15초)<br>- Scalability: 3 replicas로 높은 알림 트래픽 처리, FCM 재시도 메커니즘<br>**역할 상세**: RabbitMQ에서 이벤트 수신 → 지점주 디바이스 토큰 조회 → FCM API 호출 → Push 전송<br>**다중성 역할**: 3개 인스턴스가 RabbitMQ Consumer로 병렬 처리, 한 인스턴스 장애 시 다른 인스턴스가 이벤트 소비 |
| **MLOps Service Node (GPU Enabled)** | **역할**: AI 모델 재학습, 검증, 배포 (Hot Swap) 수행<br>**Multiplicity**: 1-2 replicas (1: Production, 1: Training 전용)<br>**H/W 사양** (per pod):<br>- CPU: 8 vCPU<br>- Memory: 16GB<br>- GPU: NVIDIA T4 × 2 (16GB VRAM each, 총 32GB)<br>- Storage: 50GB (모델 가중치 저장)<br>- Network: 10Gbps<br>**S/W 사양**: Spring Boot 3.2.x, Java 17, TensorFlow 2.13+, PyTorch 2.1+, CUDA 12.2, cuDNN 8.9<br>**배포 컴포넌트** (Interface Layer):<br>- TrainingController, DeploymentController<br>**배포 컴포넌트** (Business Layer):<br>- TrainingManager, DeploymentService, TrainingPipelineOrchestrator<br>- ModelVerificationService, DataManagementService<br>- DataCollector, DataPersistenceManager, AccuracyVerifier, PerformanceVerifier<br>**배포 컴포넌트** (System Interface Layer):<br>- ModelJpaRepository, TrainingDataJpaRepository, MLInferenceEngineAdapter<br>- RabbitMQAdapter, FaceModelClientAdapter, AuthRepositoryAdapter, HelperRepositoryAdapter<br>**QA 기여**:<br>- **Modifiability (QAS-06)**: **Hot Swap (Runtime Binding)**<br>  - **재학습**: TaskConfirmedEvent 구독 → 수정 데이터 100건+ 누적 시 자동 재학습 (2-4시간, Background)<br>  - **검증**: 정확도 (>90%), 성능 (< 200ms) 검증<br>  - **배포**: gRPC로 FaceModel Service에 신규 모델 전송 → AtomicReference로 Hot Swap (< 1ms, 무중단)<br>  - **롤백**: 1분 이내 이전 버전 복구<br>  - **결과**: 서비스 다운타임 0초, API 실패율 < 0.1% 달성<br>- Performance: GPU 가속으로 훈련 시간 단축 (CPU 대비 10배 빠름)<br>- Cost: Spot Instances 활용으로 GPU 비용 70% 절감<br>**역할 상세**:<br>- **훈련 인스턴스**: 재학습 파이프라인 실행 (데이터 수집 → 전처리 → 훈련 → 검증 → 배포)<br>- **Production 인스턴스**: gRPC 서버로 FaceModel Service에 모델 배포 요청 처리<br>**다중성 역할**: 훈련과 배포를 분리하여 배포 인스턴스는 항상 가용, 훈련 인스턴스는 필요 시 Spot Instance로 기동 |

---

## Infrastructure Zone Nodes (Private Subnet)

| Name | Description |
|------|-------------|
| **RabbitMQ Cluster** | **역할**: 메시지 브로커, 이벤트 기반 비동기 통신 중개<br>**Multiplicity**: 3 nodes (Quorum Queue)<br>**H/W 사양** (per node):<br>- CPU: 2 vCPU<br>- Memory: 4GB<br>- Storage: 50GB SSD (Message persistence)<br>- Network: 10Gbps<br>**S/W 사양**: RabbitMQ 3.12.8, Erlang 26, Kubernetes StatefulSet<br>**배포 컴포넌트** (Message Broker):<br>- MessagePublisherApi, MessageSubscriptionApi<br>- RoutingKeyResolver, QueueManager, ExchangeManager<br>- MessagePersistenceManager, QuorumQueueManager<br>**Cluster 구성**:<br>- Quorum Queue: 3-node consensus (Raft protocol)<br>- Exchange: Topic Exchange (event routing), Fanout Exchange (broadcast)<br>- Persistence: Durable queues, Message TTL: 24 hours<br>**QA 기여**:<br>- **Availability (DD-02)**: **Message-Based Communication**<br>  - Guaranteed delivery (at-least-once semantics)<br>  - Message persistence (디스크 저장) → 노드 장애 시에도 메시지 손실 없음<br>  - Quorum Queue (3-node 합의) → 1개 노드 장애에도 정상 동작<br>- Modifiability: 느슨한 결합 (Publisher와 Subscriber 독립), 새 Consumer 추가 용이<br>- Performance: 초당 10,000+ 메시지 처리<br>**역할 상세**:<br>- **Node 1**: Quorum Queue Leader, 모든 Write 처리 → Follower로 복제<br>- **Node 2**: Quorum Queue Follower, Leader 장애 시 승격<br>- **Node 3**: Quorum Queue Follower, Leader 장애 시 승격<br>**다중성 역할**: Leader 장애 시 Follower가 자동 Leader 승격 (< 5초), 2/3 노드 정상이면 클러스터 동작 |

---

## Database Zone Nodes (Private Subnet)

| Name | Description |
|------|-------------|
| **RDS Cluster** | **역할**: 각 서비스별 독립 PostgreSQL 데이터베이스 클러스터<br>**Multiplicity**: 8개 DB × (1 Primary + 2 Read Replicas) = 24 인스턴스<br>**DB 목록**: Auth DB, Helper DB, Search DB, Branch DB, Monitor DB, Vector DB, Model DB, Training Data DB<br>**H/W 사양** (per instance):<br>- CPU: 4 vCPU (db.r6g.xlarge equivalent)<br>- Memory: 16GB<br>- Storage: 500GB SSD (Auto-scaling up to 2TB)<br>- Network: 10Gbps (Private subnet)<br>- IOPS: 3,000 IOPS (Provisioned)<br>**S/W 사양**: PostgreSQL 15.4, pgvector extension (Vector DB)<br>**HA 구성**:<br>- Multi-AZ Deployment (Primary in AZ-A, Replica in AZ-B/C)<br>- Synchronous replication (Primary → Replica)<br>- Automatic failover (< 60초)<br>- Automated backup (Daily, 7-day retention, Point-in-time recovery)<br>**QA 기여**:<br>- Availability (QAS-05): Multi-AZ로 99.95% SLA, 자동 페일오버로 장애 시 Replica 승격<br>- Performance: Read Replica로 읽기 부하 분산 (Primary: Write, Replica: Read), Connection Pool (HikariCP: 10-20 per service)<br>- Security (QAS-04): Private subnet 배치 (외부 접근 불가), Encryption at rest (AES-256), Encryption in transit (TLS 1.2)<br>- Modifiability (DD-03): **Database per Service** → 각 서비스가 독립 DB 소유, 스키마 변경이 다른 서비스에 영향 없음<br>**역할 상세**:<br>- **Primary**: 모든 Write 트랜잭션 처리, Replica로 동기 복제<br>- **Read Replica 1**: 읽기 전용 쿼리 처리 (서비스 read 트래픽 분산)<br>- **Read Replica 2**: 읽기 전용 쿼리 처리 + Analytics 쿼리 (대시보드)<br>**다중성 역할**: Primary 장애 시 Replica 1이 자동 승격 (< 60초), Replica 2는 계속 읽기 전용 제공 |
| **ElasticSearch Cluster (SearchEngineDB)** | **역할**: 전문 검색 엔진, 지점/리뷰 Full-text Search 제공<br>**Multiplicity**: 3 nodes (1 Master, 2 Data nodes)<br>**H/W 사양** (per node):<br>- CPU: 4 vCPU<br>- Memory: 8GB (JVM Heap: 4GB)<br>- Storage: 200GB SSD<br>- Network: 10Gbps<br>**S/W 사양**: ElasticSearch 8.10+, Kubernetes StatefulSet<br>**배포 컴포넌트** (Search Service에서 사용):<br>- ElasticSearchRepository (ISearchEngineRepository 구현)<br>- SearchEngineDB (시스템 내부 이름)<br>**Cluster 구성**:<br>- Sharding: 5 primary shards, 1 replica shard per primary<br>- Index: branch_index (지점 정보), review_index (리뷰)<br>- Replication: Quorum-based (2/3 nodes agree)<br>**QA 기여**:<br>- **Performance (QAS-03)**: Full-text search ~50ms (RDBMS LIKE 대비 10-100배 빠름), TF-IDF/BM25 알고리즘<br>- Availability: Replica shard로 Data node 장애 시에도 검색 가능, Cluster auto-healing (Master re-election)<br>- Scalability: Horizontal scaling (Data node 추가로 용량/성능 증가)<br>**역할 상세**:<br>- **Master Node**: 클러스터 상태 관리, 샤드 할당, 인덱스 생성/삭제<br>- **Data Node 1**: Primary shard 0, 2, 4 보유 + Replica shard 1, 3<br>- **Data Node 2**: Primary shard 1, 3 보유 + Replica shard 0, 2, 4<br>**다중성 역할**: Data node 1 장애 시 Replica shard가 Primary로 승격, Master node 장애 시 Data node가 Master 역할 인수 |
| **Redis Cache Cluster** | **역할**: 인메모리 캐시, Face Vector Cache 및 Session 저장<br>**Multiplicity**: 3 nodes (1 Primary, 2 Replicas)<br>**H/W 사양** (per node):<br>- CPU: 2 vCPU<br>- Memory: 8GB (전체 메모리 사용)<br>- Network: 10Gbps<br>**S/W 사양**: Redis 7.2+, Redis Sentinel (HA), Kubernetes StatefulSet or AWS ElastiCache<br>**HA 구성**:<br>- Redis Sentinel: 3-node monitoring<br>- Automatic failover (< 5초)<br>- Persistence: RDB snapshot (hourly) + AOF (append-only file)<br>**Cache 전략**:<br>- Eviction Policy: LRU (Least Recently Used)<br>- TTL: 24 hours (Face Vectors), 30 minutes (Session)<br>- Hit Rate Target: 90%+<br>**QA 기여**:<br>- **Performance (QAS-02)**: **Face Vector Cache**<br>  - Cache hit: ~1ms (Sub-millisecond), DB 조회 50ms → 1ms (98% 감소)<br>  - Hit rate 90%+ → 대부분 요청이 DB 조회 생략<br>- Availability: Primary 장애 시 Sentinel이 Replica 승격 (< 5초)<br>**역할 상세**:<br>- **Primary**: 모든 Write 처리, Replica로 비동기 복제<br>- **Replica 1**: 읽기 전용, Primary 장애 시 승격 후보<br>- **Replica 2**: 읽기 전용, Primary 장애 시 승격 후보<br>**다중성 역할**: Primary 장애 시 Sentinel이 투표로 Replica 1 또는 2를 새 Primary로 승격, 읽기 부하는 Replica로 분산 |
| **S3 Storage** | **역할**: 객체 스토리지, 작업 사진 및 안면 이미지 저장<br>**Multiplicity**: N/A (관리형 서비스, 무제한 확장)<br>**Provider**: AWS S3 / GCP Cloud Storage<br>**Storage 구성**:<br>- Bucket: task-photos (작업 사진), face-images (안면 사진), ml-models (모델 가중치)<br>- Storage Classes:<br>  - Standard: 최근 30일 사진 (빠른 액세스)<br>  - Infrequent Access: 30-90일 사진<br>  - Glacier: 90일 이상 사진 (아카이브)<br>- Lifecycle Policy: 자동 아카이빙 (Standard → IA → Glacier)<br>- Versioning: Enabled (실수로 삭제 방지)<br>- Replication: Cross-region replication (DR)<br>**QA 기여**:<br>- Availability: 99.99% SLA, 11-nines durability (99.999999999%)<br>- Scalability: 무제한 확장 (Exabyte 규모), 초당 5,500+ GET req per prefix<br>- Security (QAS-04): Encryption at rest (AES-256), Encryption in transit (TLS 1.2), Bucket Policy (IP Whitelist), Pre-signed URL (임시 접근)<br>- Cost: Lifecycle Policy로 비용 70% 절감 (Standard → Glacier)<br>**역할 상세**: Helper/Auth/MLOps Service가 사진 업로드 → S3 저장 → Pre-signed URL 반환 → 필요 시 다운로드 |

---

## Summary Table

| Zone | Node Count | Total Instances | Key QA Contribution |
|------|-----------|----------------|---------------------|
| **Client Zone** | 4 types | 11,000+ devices | Security (TLS), Usability |
| **Branch Zone** | 1 type | 1,000+ devices | Performance (로컬 처리), Availability (Heartbeat) |
| **External Services** | 3 types | 3 providers | Security (PG), Performance (LLM Cold Path), Availability (FCM) |
| **Kubernetes Cluster** | 10 types | 24 replicas | Performance (Co-location), Availability (HA), Modifiability (Hot Swap) |
| **Infrastructure Zone** | 1 type | 3 nodes | Availability (DD-02), Modifiability (Message-Based) |
| **Database Zone** | 4 types | 34 instances | Availability (Multi-AZ), Performance (Cache), Modifiability (DB per Service) |
| **Total** | 24 types | ~12,064 instances | All QA attributes |

---

## Key QA Achievement Summary by Node

| QA Attribute | Critical Nodes | Contribution |
|--------------|----------------|--------------|
| **Performance (QAS-02)** | Real-Time Access Node (Co-located), Redis Cache | IPC/gRPC (< 10ms), Cache (98% 감소) |
| **Performance (QAS-03)** | Search Service Node, ElasticSearch Cluster | Hot/Cold Path, Full-text search (~50ms) |
| **Availability (QAS-01)** | Monitoring Service Node, Notification Service Node | Two-Level Detection, FCM Push (< 15초) |
| **Availability (QAS-05)** | Load Balancer, RDS Cluster, RabbitMQ Cluster (Infrastructure Zone) | Active-Active HA, Multi-AZ, Auto-failover |
| **Security (QAS-04)** | API Gateway Node, RDS Cluster, S3 Storage | TLS 1.3, Encryption at rest, Private subnet |
| **Modifiability (QAS-06)** | MLOps Service Node, RDS Cluster | Hot Swap, Database per Service |

