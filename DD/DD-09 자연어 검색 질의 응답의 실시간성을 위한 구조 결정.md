

**QAS-03 (자연어 검색 3초 이내 응답)** [cite: 511]은 우리 시스템의 '지능형' 가치를 고객이 직접 체감하는 첫 관문이자, **성능(Performance)** 품질 속성의 핵심 시나리오입니다.

학생이 지적한 대로, 이 3초 SLA는 단순한 DB 조회가 아니라 **'상용 LLM 서비스'** [cite: 251]라는 **외부 의존성**을 포함하고 있어 난이도가 매우 높습니다. 만약 외부 LLM 서비스가 응답하는 데 2.9초가 걸린다면, 우리 시스템은 네트워크 지연 시간 0.1초 만에 모든 처리를 끝내야 하므로 사실상 SLA를 위반하게 됩니다. [cite: 178]

교수로서 이 '3초의 벽'을 넘기 위한 3가지 주요 디자인 어프로치를 제시하겠습니다. 각 방식은 서로 다른 아키텍처 택틱을 사용하며, 명확한 트레이드오프(Trade-off)를 가집니다.

---

## 🏛️ Approach 1: 동기식 파이프라인 (The "Hopeful" Approach)

가장 구현이 간단하지만, SLA 달성 여부를 **외부 LLM의 성능에 전적으로 의존**하는 방식입니다.

* **설계 (Flow):**
    1.  [API Gateway] -> `SearchManager`
    2.  `SearchManager`가 **즉시** `ILLMAnalysisService` (외부)를 **동기(Sync) 호출**합니다. [cite: 405]
    3.  (N초 대기...)
    4.  LLM이 키워드를 반환하면, `ISearchRepository` (내부 DS-07)를 쿼리합니다. [cite: 406]
    5.  결과를 사용자에게 반환합니다.

* **아키텍처 택틱:** `Pipe and Filter` (파이프 및 필터) [cite: 14]
* **장점:**
    * 구현이 매우 단순하고 직관적입니다. (`SearchManager.java`의 현재 로직과 유사)
    * 모든 쿼리에 대해 항상 LLM의 최신 분석을 받을 수 있습니다.
* **단점 (치명적):**
    * **QAS-03 SLA(3초)를 보장할 수 없습니다.** [cite: 511]
    * 외부 LLM의 성능 변동(Latency spike)이 우리 서비스의 장애로 **즉시 전파**됩니다.
    * 외부 LLM 호출 비용이 쿼리마다 발생하여 비용 효율성이 낮습니다.

---

## 🏛️ Approach 2: 쿼리 캐시 (The "Optimistic Cache" Approach)

사용자들이 자주 묻는 질문은 동일하다는 가정 하에, **LLM 호출 결과를 캐시**하여 응답 속도를 높이는 방식입니다.

* **설계 (Flow):**
    1.  [API Gateway] -> `SearchManager`
    2.  `SearchManager`가 사용자의 원본 쿼리(e.g., "샤워실이 넓은 곳")를 Key로 **캐시(e.g., Redis)를 조회**합니다.
    3.  **(Cache Hit):** 캐시된 키워드/결과가 있다면, LLM을 호출하지 않고 즉시 캐시된 결과(또는 키워드로 DS-07 재조회)를 반환합니다. **(Fast Path, 0.1초)**
    4.  **(Cache Miss):** 캐시에 없다면, *Approach 1*처럼 **동기식으로 LLM을 호출**합니다. **(Slow Path, N초)**
    5.  LLM 응답 및 DS-07 조회 결과를 **캐시에 저장**한 후 사용자에게 반환합니다.

* **아키텍처 택틱:** `Page Cache` (페이지 캐시) [cite: 61]
* **장점:**
    * "Fat Head" (인기 있는) 쿼리에 대해서는 **QAS-03 SLA를 월등히 초과 달성**합니다.
    * *Approach 1*에 비해 LLM 호출 비용과 외부 의존성을 크게 줄일 수 있습니다.
* **단점:**
    * **"Long Tail" (새롭거나 비주류인) 쿼리**에 대해서는 *Approach 1*과 동일하게 **SLA를 위반**합니다.
    * 캐시를 관리(eviction, refresh)해야 하는 복잡성이 추가됩니다.

---

## 🏛️ Approach 3: 비동기 사전 인덱싱 (The "Guaranteed" Approach)

LLM처럼 느린 작업을 **실시간 요청 경로(Hot Path)에서 완전히 제거**하고, **비동기 인덱싱 경로(Cold Path)**로 분리하는 가장 성숙한 방식입니다.

* **설계 (Flow):**
    * **[Cold Path: 쓰기/인덱싱 시 (비동기)]**
        1.  지점주가 리뷰(UC-10)나 지점 정보(UC-18)를 등록하면, `SearchManager`는 `IMessagePublisherService`를 통해 **이벤트를 발행**하고 즉시 응답합니다.
        2.  별도의 `PreferenceMatchConsumer`가 이 이벤트를 구독합니다.
        3.  `Consumer`가 **시간 여유를 가지고** 외부 `ILLMAnalysisService`를 호출하여 해당 콘텐츠의 키워드/성향을 추출합니다.
        4.  추출된 키워드/성향을 검색 엔진(DS-07)에 **미리 인덱싱**합니다.

    * **[Hot Path: 검색 시 (동기)]**
        1.  [API Gateway] -> `SearchManager`
        2.  `SearchManager`는 **절대 외부 LLM을 호출하지 않습니다.**
        3.  사용자 쿼리("샤워실이 넓은 곳")에서 **단순 키워드(e.g., "샤워실", "넓다")를 자체 추출**하거나, 이미 모든 정보가 인덱싱된 DS-07의 **'Full-Text Search'** 기능을 활용해 즉시 쿼리합니다.
        4.  미리 인덱싱된 결과를 **즉시 반환**합니다. **(Fast Path, 0.5초)**

* **아키텍처 택틱:** `Event Based` (이벤트 기반) [cite: 23] + `Message Based` (메시지 기반) [cite: 25]
* **장점:**
    * **QAS-03 SLA(3초)를 100% 보장**할 수 있는 유일한 아키텍처입니다.
    * 실시간 검색 경로는 외부 의존성이 전혀 없으므로 **장애 전파(Fault propagation)가 원천 차단**됩니다.
* **단점:**
    * 구현 복잡성이 가장 높습니다. (메시지 큐, Consumer 관리 필요)
    * 사용자의 미묘한 자연어 *의도*("시끄럽지 않은 곳")를 단순 키워드 검색으로 완벽히 매칭하기 어려워, 검색 *품질*이 다소 저하될 수 있습니다. (이 경우, Hot Path에서도 경량화된 로컬 모델을 사용하기도 합니다.)

---

### 💡 교수의 제언

학생이 업데이트한 `SearchServiceComponent.puml`은 **Approach 1(동기식 LLM 호출)**과 **Approach 3(비동기 인덱싱)**의 일부가 혼재된 **하이브리드** 형태를 보입니다. 이는 실시간 검색(UC-09)은 *Approach 1*을, 비동기 알림(UC-11)은 *Approach 3*을 따르는 것으로 해석됩니다.

이 경우, 여전히 실시간 검색(UC-09)은 **QAS-03의 SLA 실패 위험**을 안고 있습니다.

QAS-03을 '보장'하기 위한 아키텍처는 **Approach 3**입니다.
만약 캐시 히트율(Hit Rate)을 신뢰할 수 있다면 **Approach 2**가 가장 실용적인 절충안이 될 수 있습니다.

이 트레이드오프(SLA 보장 vs. 구현 복잡성)를 명확히 인지하고 다음 설계를 진행하기 바랍니다.